In this web scraping task, I ensured adherence to the website's crawling policies. First, I checked the website's `robots.txt` file to determine which URLs were allowed for crawling. If a sitemap was available, I used the `requests` library and BeautifulSoup to parse the sitemap and gather a structured list of accessible URLs. For each of these URLs, I scraped data such as URLs, titles, and headings using BeautifulSoup which is stored in storeddata.txt file . Throughout the process, I made sure to respect the rules specified in the `robots.txt` file to ensure compliance with the websiteâ€™s policies. I stored the extracted data for further use or analysis. 
